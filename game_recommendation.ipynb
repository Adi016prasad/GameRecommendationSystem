{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3938c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "import json\n",
    "title1=[]\n",
    "product_id1=[]\n",
    "category1=[]\n",
    "description1=[]\n",
    "link1=[]\n",
    "DEVICE_CATEGORIES = ['windows', 'tv', 'tablet', 'phone', 'chromebook']\n",
    "\n",
    "for i in DEVICE_CATEGORIES:\n",
    "    \n",
    "    params = {\n",
    "        'api_key': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx',#put your serp api keys here\n",
    "        'engine': 'google_play',\n",
    "        'store': 'games',  # Explicitly set the category to games\n",
    "        'store_device': i,  # Specify device type\n",
    "         }\n",
    "\n",
    "    search = GoogleSearch(params)   # where data extraction happens on the SerpApi backend\n",
    "    result_dict = search.get_dict() # JSON -> Python dict\n",
    "\n",
    "    #print(result_dict)\n",
    "    google_play_games = result_dict['organic_results']\n",
    "    \n",
    "    \n",
    "    for j in range(len(google_play_games)):\n",
    "        for k in range(len(google_play_games[j][\"items\"])):\n",
    "            title1.append(google_play_games[j][\"items\"][k][\"title\"])\n",
    "            product_id1.append(google_play_games[j][\"items\"][k][\"product_id\"])\n",
    "            category1.append(google_play_games[j][\"items\"][k][\"category\"])\n",
    "            description1.append(google_play_games[j][\"items\"][k][\"description\"])\n",
    "            link1.append(google_play_games[j][\"items\"][k][\"link\"])\n",
    "        \n",
    "    \n",
    "    \n",
    "    print(f'{i}done*************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48040cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "dataset= pd.DataFrame({\n",
    "    'title': title1,\n",
    "    'product_id':product_id1,\n",
    "    'category': category1,\n",
    "    'description':description1,\n",
    "    'link': link1\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ee11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(r'C:\\Users\\Dell\\3D Objects\\NLP\\nowgg_dataset3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebed922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset1=pd.read_csv(\"nowgg_dataset3.csv\")\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2=dataset1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2['category'] = dataset2['category'].str.replace(' ', '')#remove spaces between words to capture semantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af974279",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2[\"category\"]=dataset2[\"category\"].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e3f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here the cleaning of description columns take place\n",
    "import ast\n",
    "import string\n",
    "import re\n",
    "\n",
    "def cleaning(text):\n",
    "    \n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text3=text.translate(translator)#punctuations\n",
    "    text1=text3.lower()#lower\n",
    "    text2 = re.sub(r'\\d+', '', text1)\n",
    "    text4=\" \".join(text2.split())#join\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text5= emoji_pattern.sub(r'', text4)\n",
    "    text5=text5.replace('â€¢','')\n",
    "    \n",
    "    \n",
    "    dem = demoji.findall(text5)\n",
    "\n",
    "    for item in dem.keys():\n",
    "\n",
    "        text5 = text5.replace(item,'')\n",
    "        \n",
    "    text5=re.sub(\"[^a-z]\",' ',text5)\n",
    "    \n",
    "\n",
    "    return text5\n",
    "\n",
    "\n",
    "dataset2[\"description\"]=dataset2[\"description\"].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2[\"description\"]=dataset2[\"description\"].apply(lambda x:x.split()[:100])\n",
    "dataset2[\"category\"]=dataset2[\"category\"].apply(lambda x:x.split())\n",
    "dataset2[\"final_description\"]=dataset2[\"category\"]+dataset2[\"description\"]\n",
    "dataset2=dataset2.drop([\"category\",\"description\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c081ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2[\"final_description\"]=dataset2[\"final_description\"].apply(lambda x:\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a4224",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4031cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3=dataset2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5068bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = dataset3.reset_index(drop=True)\n",
    "dataset3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7987833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3[\"embeddings\"]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere           \n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Cohere client\n",
    "cohere_api_key = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'  # Replace with your Cohere API key\n",
    "co = cohere.Client(cohere_api_key)\n",
    "\n",
    "\n",
    "# Function to get embeddings using Cohere\n",
    "def get_cohere_embedding(text):\n",
    "    response = co.embed(texts=[text])\n",
    "    return response.embeddings[0][:250]\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for index,row in dataset3.iterrows():\n",
    "    if row[\"final_description\"]!=\"NaN\":\n",
    "        embedding = get_cohere_embedding(row['final_description'])\n",
    "        embeddings.append(embedding)\n",
    "        print(f\"{index}done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cca4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(embeddings)):\n",
    "    dataset3.at[i,\"embeddings\"]=embeddings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5244b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8fca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3.to_csv(r'C:\\Users\\Dell\\3D Objects\\NLP\\nowgg_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099bec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Convert embeddings to a numpy array\n",
    "embedding_matrix = np.array(dataset3['embeddings'].tolist()).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ee1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FAISS index\n",
    "dimension = embedding_matrix.shape[1]  # Dimensionality of the embeddings\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(embedding_matrix)\n",
    "\n",
    "# Check the number of elements in the index\n",
    "print(f\"Number of embeddings indexed: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ac8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac8d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
